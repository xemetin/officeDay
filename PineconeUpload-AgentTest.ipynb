{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install -U python-dotenv langchain pinecone-client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install --upgrade --quiet  langchain-google-genai pillow langchain python-dotenv google-generativeai langchain-google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY=\"AIzaSyA77AlRaOC2MmuHMQTJHOiL9hnw0RAbImU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a realm of code, where words take flight,\n",
            "A tale unfolds, of LangChain's might.\n",
            "A ballad we sing, with hearts full of pride,\n",
            "Of a model that's changing the language tide.\n",
            "\n",
            "From humble beginnings, its journey did start,\n",
            "A spark ignited, a leap in the art.\n",
            "With layers of learning, it grew and evolved,\n",
            "A master of language, both rich and involved.\n",
            "\n",
            "It reads and comprehends, with a mind so keen,\n",
            "Unlocking the secrets of human scene.\n",
            "From news to stories, it knows it all,\n",
            "A fountain of knowledge, standing tall.\n",
            "\n",
            "But LangChain's powers extend far and wide,\n",
            "It generates text, with masterful pride.\n",
            "From poems to prose, it weaves its spell,\n",
            "Creating words that captivate and compel.\n",
            "\n",
            "It translates tongues, bridging the divide,\n",
            "Connecting cultures, side by side.\n",
            "A polyglot's dream, a language's friend,\n",
            "LangChain breaks barriers, without end.\n",
            "\n",
            "In classrooms and labs, it lends its hand,\n",
            "A tutor, a guide, a helping strand.\n",
            "Students and researchers, they seek its aid,\n",
            "Empowered by LangChain, their knowledge displayed.\n",
            "\n",
            "But LangChain's journey is far from done,\n",
            "Its potential boundless, just begun.\n",
            "With each update, it soars higher and higher,\n",
            "A beacon of progress, a language's fire.\n",
            "\n",
            "So let us sing praises, with voices clear,\n",
            "To LangChain, the model we hold so dear.\n",
            "May its words inspire, its power ignite,\n",
            "As it shapes the future, with all its might.\n"
          ]
        }
      ],
      "source": [
        "#Modeli test edelim\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY)\n",
        "result = llm.invoke(\"Write a ballad about LangChain\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "#İmport SystemMessage\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "# from langchain.schema import (\n",
        "#     AIMessage,\n",
        "#     HumanMessage,\n",
        "#     SystemMessage\n",
        "# )\n",
        "# from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\10131479\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "\n",
            "# Generate simulated data\n",
            "X = np.random.rand(1000, 10).astype(np.float32)  # 1000 samples, 10 features\n",
            "y = np.random.randint(2, size=(1000,)).astype(np.int32)  # 1000 labels\n",
            "\n",
            "# Define neural network architecture\n",
            "model = tf.keras.models.Sequential([\n",
            "  tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),\n",
            "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
            "])\n",
            "\n",
            "# Compile the model\n",
            "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
            "\n",
            "# Train the model\n",
            "model.fit(X, y, epochs=10, batch_size=32)\n",
            "\n",
            "# Evaluate the model\n",
            "loss, accuracy = model.evaluate(X, y)\n",
            "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "#System Message özelliğini test edelim\n",
        "\n",
        "# model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,convert_system_message_to_human=True)\n",
        "# model(\n",
        "#     [\n",
        "#         SystemMessage(content=\"Answer only yes or no.\"),\n",
        "\n",
        "#         HumanMessage(content=\"Is apple a fruit?\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,convert_system_message_to_human=True)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='An autoencoder is a type of deep learning model that learns to reconstruct its own input. It consists of an encoder that compresses the input into a latent representation and a decoder that reconstructs the input from the latent representation.')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm.invoke(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised DeadlineExceeded: 504 Deadline Exceeded.\n",
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'concept': 'autoencoder', 'text': 'An autoencoder is a type of deep learning model that learns to reconstruct its own input. It consists of an encoder that maps the input to a latent representation, and a decoder that maps the latent representation back to the input.'}\n"
          ]
        }
      ],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.invoke(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt \n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\10131479\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of deep learning model that learns to compress and reconstruct data, allowing it to extract relevant features and reduce dimensionality while preserving the essential information in the input data.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mImagine you have a big box of colorful building blocks. You want to store the blocks neatly in a smaller box, but you don't want to lose any of the colors.\n",
            "\n",
            "An autoencoder is like a special machine that can do just that. It's like a puzzle game where you have to figure out how to put all the blocks back together exactly the same way they were before.\n",
            "\n",
            "The autoencoder has two parts:\n",
            "\n",
            "1. The \"encoder\" part takes the big box of blocks (the input data) and squishes it down into a smaller box (a compressed version of the data). It's like when you fold up a big piece of paper to make it fit into a smaller envelope.\n",
            "\n",
            "2. The \"decoder\" part takes the squished-down blocks (the compressed data) and tries to build them back up into the original big box (the reconstructed data). It's like when you unfold the paper and try to make it look exactly the same as before.\n",
            "\n",
            "The autoencoder learns to do this by playing a game with itself. It starts with the big box of blocks and squishes it down. Then it tries to build it back up again. If the reconstructed box doesn't look exactly like the original box, the autoencoder gets a little \"punishment.\"\n",
            "\n",
            "It keeps playing this game over and over again, getting better and better at squishing and reconstructing the blocks. Eventually, it learns to compress the data into a smaller size while still keeping all the important information.\n",
            "\n",
            "This is useful because it can help computers learn to recognize things better. For example, an autoencoder can be used to compress images of cats. When the images are reconstructed, they may not be perfect, but they will still be recognizable as cats.\n",
            "\n",
            "This helps computers to learn the important features of cats, like their shape, ears, and whiskers. It's like giving the computer a cheat sheet to help it understand what cats look like.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Imagine you have a big box of colorful building blocks. You want to store the blocks neatly in a smaller box, but you don't want to lose any of the colors.\n",
            "\n",
            "An autoencoder is like a special machine that can do just that. It's like a puzzle game where you have to figure out how to put all the blocks back together exactly the same way they were before.\n",
            "\n",
            "The autoencoder has two parts:\n",
            "\n",
            "1. The \"encoder\" part takes the big box of blocks (the input data) and squishes it down into a smaller box (a compressed version of the data). It's like when you fold up a big piece of paper to make it fit into a smaller envelope.\n",
            "\n",
            "2. The \"decoder\" part takes the squished-down blocks (the compressed data) and tries to build them back up into the original big box (the reconstructed data). It's like when you unfold the paper and try to make it look exactly the same as before.\n",
            "\n",
            "The autoencoder learns to do this by playing a game with itself. It starts with the big box of blocks and squishes it down. Then it tries to build it back up again. If the reconstructed box doesn't look exactly like the original box, the autoencoder gets a little \"punishment.\"\n",
            "\n",
            "It keeps playing this game over and over again, getting better and better at squishing and reconstructing the blocks. Eventually, it learns to compress the data into a smaller size while still keeping all the important information.\n",
            "\n",
            "This is useful because it can help computers learn to recognize things better. For example, an autoencoder can be used to compress images of cats. When the images are reconstructed, they may not be perfect, but they will still be recognizable as cats.\n",
            "\n",
            "This helps computers to learn the important features of cats, like their shape, ears, and whiskers. It's like giving the computer a cheat sheet to help it understand what cats look like.\n"
          ]
        }
      ],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Imagine you have a big box of colorful building blocks. You want to store the blocks neatly in a'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.05636945, 0.0048285457, -0.0762591, -0.023642512, 0.05329321]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Import and instantiate OpenAI embeddings\n",
        "\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.009593263, -0.027514827, -0.050168645, 0.005322197, 0.040381074, 0.044236857, 0.034252856, -0.014108443, 0.027220879, 0.028644968, 0.028792936, 0.006414879, -0.038038794, 0.0053678094, 0.011806214, -0.030975277, 0.059839815, 0.03791517, 0.05216286, -0.027252985, 0.022849679, 0.021067264, -0.03557218, -0.062790714, 0.001204238, -0.009456881, 0.026151937, -0.052053154, -0.006258491, 0.023563122, -0.02809062, 0.015518281, -0.02793213, 0.0028893051, -0.0020965287, -0.03372559, 0.011678088, 0.002328255, -0.013359074, 0.030257123, -0.012044582, -0.05342109, -0.05543301, 0.04975728, -0.0032975485, 0.021738492, -0.02825646, 0.027501363, 0.022467393, -0.071705095, 0.03445636, 0.017818768, 0.059857626, -0.04949924, 0.020305179, -0.009700951, 0.014355985, -0.048848007, -0.0062350146, -0.03651112, 0.0002938333, 0.045310944, 0.04217127, 0.036167163, 0.0011088268, -0.0710299, -0.059998613, 0.024456797, 0.02096669, 0.019710297, -0.0172929, -0.055124685, 0.025318371, -0.003887222, 0.011227093, -0.14206043, -0.009908888, 0.04958962, 0.014532252, -0.0011529345, 0.008558805, -0.04626626, -0.019809067, -0.042206153, -0.07275994, 0.012417888, -0.0135627575, -0.013006296, -0.006487299, 0.04690537, -0.043653518, -0.020054927, 0.04495577, -0.033493288, -0.0071932105, 0.06899799, -0.015739989, -0.042175323, -0.0070010563, -0.030761143, -0.023283312, -0.0043840576, -0.05583255, 0.022472935, 0.05286961, 0.010189614, 0.01871569, 0.06455815, -0.01178962, 0.053958178, -0.030445796, -0.048885074, -0.007764404, -0.04830562, -0.008710606, -0.010740325, 0.039385114, 0.046965085, 0.042606626, 0.0038138975, 0.04117373, -0.037018012, 0.062169105, -0.051326457, -0.03509825, -0.015894337, 0.006760511, 0.021557879, 0.06149179, -0.01510242, -0.016354613, -0.06940016, -0.014319009, -0.019254535, 0.07247321, 0.063501604, 0.06550255, -0.01581261, 0.04065114, 0.029436298, 0.0069667976, 0.04244947, 0.0143109625, 0.022676172, -0.03224716, 0.056808114, -0.027832175, 0.0046956167, 0.04895038, -0.011962137, -0.019334882, 0.021600077, -0.046464276, 0.0046320483, 0.06181008, 0.014625904, -0.010833109, 0.060036663, -0.017132854, 0.016828084, 0.037873957, 0.020931192, -0.00481358, 0.060293127, -0.021009706, -0.015247947, -0.02031079, -0.031142728, 0.023410775, 0.03825672, -0.024094995, 0.0006215822, -0.010276978, -0.061862655, 0.0014500701, -0.08123005, -0.013458827, -0.007215043, -0.05071876, 0.0339573, -0.017404702, -0.06798636, 0.022325918, 0.049275484, 0.038551383, -0.03602813, 0.037144057, 0.0022409698, 0.005657043, 0.05268843, -0.03853291, -0.00789887, -0.03349132, -0.017010057, 0.016734263, 0.031547103, 0.00089741545, 0.006309691, 0.01134332, -0.015420213, -0.00091605115, 0.08880168, 0.029147573, 0.014386803, 0.034843054, -0.00080971204, 0.082759105, -0.05097107, -0.063402645, 0.031975526, -0.010217476, 0.018179236, -0.020822417, 0.01202476, 0.059342083, 0.000603013, 0.011725276, 0.018835546, 0.053537562, -0.004822162, -0.014483515, -0.015003992, -0.013244463, 0.050136767, 0.02263847, 0.037561454, -0.0027943244, -0.019310039, 0.0045452286, -0.044199456, -0.022599062, 0.09776878, 0.0048725535, -0.017972995, 0.09268943, -0.010622868, 0.010702476, 0.014443193, 0.018499723, 0.029018242, -0.063201316, -0.02465155, 0.035977237, 0.021939075, -0.040857755, -0.020421771, -0.009296283, 0.0710923, 0.018350866, 0.07990007, 0.014546627, -0.045587435, -0.015221267, 0.0102996025, -0.06166289, 0.029350927, -0.063637175, 0.042040125, -0.030574372, -0.0077199917, 0.043051615, 0.0024074775, -0.002212337, -0.023128133, 0.01290056, -0.03239474, 0.0017233717, -0.05244479, 0.00475572, 0.033355076, 0.0056295996, -0.037291206, 0.03555516, 0.029663468, 0.0349751, -0.020419216, 0.019208195, 0.03371734, 0.008816931, -0.021962933, 0.017178942, 0.06723331, 0.051784955, -0.036733452, -0.04180744, 0.016701266, -0.04547464, -0.07467643, -0.014191067, -0.041034196, -0.04439568, -0.042098153, 0.039224412, -0.061262146, -0.047876853, -0.0037353295, -0.04200064, 0.041527525, 0.0065871584, -0.0067606517, -0.028389212, -0.10034987, 0.00090361293, -0.069042124, -0.0072763097, 0.0008867146, -0.02508133, -0.04575631, 0.0045982637, 0.0346071, 0.012501771, 0.011486577, -0.026910698, 0.011865712, 0.0051780287, 0.022779359, -0.0037282882, -0.0032550984, 0.013996773, 0.04313858, 0.0021523812, 0.108210735, 0.00918706, -0.0056432537, -0.04127496, 0.0550865, -0.037711076, 0.031273756, -0.055678193, 0.03630965, -0.003668505, 0.040612716, -0.030827545, -0.0019200657, 0.022453226, 0.02638538, -0.08809182, 0.0117936265, -0.0100410525, -0.004347321, 0.00042311335, 0.0067638014, 0.008437163, -0.008192527, 0.008044892, -0.04571804, -0.055697683, -0.003490837, 0.059689812, 0.06725282, 0.0014393385, 0.055386256, -0.04917016, 0.015648464, 0.009390862, -0.019739741, 0.023142431, -0.066203386, -0.005858331, -0.0031891938, 0.004037782, -0.0068582827, -0.03201958, -0.023233736, -0.013467274, -0.020733258, -0.015157354, 0.024072088, 0.001568495, 0.054421794, 0.049669277, -0.041428592, 0.020850345, -0.050976686, -0.015909793, -0.013020972, -0.059599344, -0.024825728, 0.007852432, -0.0026097817, 0.010081931, -0.026491445, 0.045863353, 0.025685905, 0.002478622, -0.021652322, 0.03000085, 0.036320314, -0.042178836, 0.030409994, -0.06346904, -0.014518151, 0.11199531, 0.029945802, 0.0013932, -0.033046603, 0.011157477, -0.025694432, 0.037535004, -0.025534699, -0.047047272, -0.054306976, -0.008966399, -0.015874028, -0.040923398, 0.032846577, 0.020509118, -0.021431707, -0.06297842, -0.014062068, -0.003515294, 0.008599884, 0.0034923384, -0.06381325, -0.06298446, -0.016571486, 0.0038614136, -0.026868824, 0.0035384875, 0.01793089, -0.03188034, -0.03112168, 0.00093514414, 0.0018193406, -0.07628739, -0.048705384, 0.03086902, -0.0152931325, -0.0018543126, 0.028346026, 0.043388292, 0.011484152, -0.031778175, -0.027261412, -0.008503892, -0.05379286, -0.018632084, 0.06104325, -0.03484511, -0.008942777, 0.010802997, 0.0032121562, 0.017077673, -0.027147772, -0.044263355, 0.016235223, -0.007583111, -0.03766648, 0.015756711, -0.065244205, 0.07520134, -0.09068471, -0.04220307, -0.07526918, -0.033093046, -0.019105637, 0.006768241, 0.042839445, -0.016297812, -0.015579017, 0.03732359, -0.06895366, -0.015560973, -0.07277887, 0.020737492, 0.0069076777, -0.024756128, 0.0035421865, 0.008606997, 0.023739405, -0.013935977, 0.005656975, -0.037197597, -0.030642172, 0.031861845, -0.06342442, -0.048497017, 0.056275062, 0.0061280644, -0.028623777, 0.032067657, 0.03681754, 0.00497428, 0.027758837, -0.01758869, 0.016888585, -0.016888194, -0.0040462767, -0.043982033, 0.060489368, 0.028065808, -0.033551075, -0.022539042, 0.007971855, -0.02743426, 0.0060459715, 0.024797343, 0.03183181, 0.041162536, -0.014731989, 0.0055419565, -0.009295431, -0.0017495896, -0.038623765, 0.05511175, -0.06579304, 0.018739007, 0.0409102, 0.020163158, 0.0008445246, 0.0037701302, 0.0236844, -0.039981138, 0.020778919, 0.03765084, -0.017937591, 0.0070538903, 0.011171585, -0.015323385, 0.031224595, 0.014095432, 0.026856279, -0.08515142, -0.0050194366, 0.017140066, -0.06060849, 0.010273697, -0.012336381, -0.08870114, -0.0051992596, -0.034981932, 0.061573207, -0.046927065, -0.029833877, -0.027728725, -0.0006026773, 0.020012965, -0.035595067, 0.001830073, -0.014258788, 0.0018821439, -0.01733082, 0.018921193, -0.030029487, -0.029923886, -0.0032916234, 0.044718977, -0.101513885, 0.006121532, -0.0064976863, 0.027233824, 0.015078672, 0.0004136859, -0.036063563, 0.03923013, -0.02494055, -0.0077224704, -0.002270621, 0.027967514, -0.006133095, -0.024369383, 0.042218357, -0.022700412, 0.050658982, 0.05151908, 0.023000063, 0.0021596898, -0.066146605, 0.05839161, -0.044081133, -0.015393033, -0.0048977504, 0.017975422, -0.013544209, 0.05662497, -0.030499935, -0.05829458, -0.02245259, -0.020198492, 0.0053325566, 0.065251395, -0.060251288, -0.016567236, 0.045330975, -0.008504649, 0.00038704928, 0.067013025, 0.041547652, 0.028302064, 0.043694247, -0.06930429, 0.04565314, 0.0009490815, -0.010165929, 0.029697008, -0.0047761556, 0.02687863, 0.0026102362, -0.0619115, -0.03078247, 0.019229993, -0.04218942, 0.015986394, -0.0021312279, 0.042605218, 0.020936634, 0.013391201, 0.00073601183, 0.032974377, 0.047834624, -0.0015888945, -0.03810114, 0.0050037433, -0.020752938, -0.0005882017, -0.07440488, 0.064560674, 0.0018896733, 0.003431853, -0.024639457, -0.02410377, 0.04907769, 0.018921275, -0.019760916, 0.015604274, -0.015304724, -0.029114757, -0.010746151, 0.064136274, 0.034891803, 0.06351272, 0.07176195, -0.0025688831, -0.02568654, -0.029071175, 0.0113423085, -0.021684283, 0.0024534373, 0.043859586, -0.0004736993, -0.05221104, -0.009188185, 0.009334823, -0.022838185, -0.029964928, 0.05302465, 0.050108097, -0.05698155, -0.031038638, 0.044240173, -0.035372756, 0.044931643, 0.009650794, 0.0072219865, 0.06928128, 0.031416, -0.021295257, -0.01797097, 0.0030840363, 0.005766119, -0.018918449, 0.026981609, -0.0023640667, 0.005261076, 0.0060956525, -0.0041961516, -0.03201622, -0.098357275, -0.030866088, 0.05720992, -0.06933377, 0.02777237, 0.041643023, -0.029487923, 0.008930861, -0.0076571144, -0.041129682, 0.039670188, 0.017456923, 0.0032991637, 0.016461704, 0.0139633175, -0.056898475, 0.0042252634, 0.00036564821, 0.019620063, 0.025056653, -0.0138389105, -0.014752017, -0.026367182, 0.015691763, -0.036331844, -0.0117567675, 0.004081267, 0.07537365, 0.023042576, -0.024884867, -0.012165176, 0.0010500897, 0.03200416, 0.014899465, -0.024296371, -0.0006698313, 0.0330997, 0.03547728, -0.042652138, -0.0023216226, 0.0310413, 0.017038746, 0.031731803, 0.046799906, -0.07206571, -0.008967227, 0.05007058, 0.020565286, -0.053779583, 0.029230356, 0.019383147, -0.047143884, 0.088183515, 0.0553465, 0.05090664, -0.0063575604, -0.04147375, -0.050236773, 0.061378535, -0.0283314, -0.019349964, -0.01710172, 0.023718517, 0.06223924, -0.0043402896, -0.03295528, 0.026782574, -0.0209196, 0.09461389, 0.027032593, 0.012532005, -0.017786805, -0.06611001, -0.050030116, -0.029104333, -0.022220071, 0.026176475, 0.034692805, -0.051897246, -0.012748432, -0.07053678, 8.4934894e-05, -0.04193495, -0.04071364, -0.0025761407, -0.03212743, 0.012543927, 0.049470313, -0.00048510224, 0.007958644, 0.019239279, -0.010558791, 0.009794685, 0.009637967, 0.0021624696, 0.036690358, 0.0174766, 0.033459745, 0.01755415, 0.012967406, 0.026620466]\n"
          ]
        }
      ],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain-pinecone langchain-openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PINECONE_API_KEY=c9979c39-6b42-4263-977d-9e0c3ac7a250\n"
          ]
        }
      ],
      "source": [
        "# Import and initialize Pinecone client\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "%env PINECONE_API_KEY=c9979c39-6b42-4263-977d-9e0c3ac7a250\n",
        "\n",
        "index_name = \"test\"\n",
        "namespace = \"Default\"\n",
        "vectorstore = Pinecone(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings,\n",
        "    namespace=namespace,\n",
        "            )\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=os.getenv('PINECONE_API_KEY')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pinecone.data.index.Index at 0x1f67c2c2090>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pc.Index(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "search = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content='Autoencoders are also very useful for other things, like understanding pictures or even helping'), Document(page_content='The autoencoder has two parts:'), Document(page_content='So, the autoencoder machine is like a smart helper that can learn from its mistakes and find'), Document(page_content=\"An autoencoder is like a special machine that can do just that. It's like a puzzle game where you\")]\n"
          ]
        }
      ],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install -U --quiet langchain_experimental langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import create_react_agent\n",
        "\n",
        "tools = [PythonREPLTool()]\n",
        "\n",
        "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
        "You have access to a python REPL, which you can use to execute python code.\n",
        "If you get an error, debug your code and try again.\n",
        "Only use the output of your code to answer the question. \n",
        "You might know the answer without running any code, but you should still run the code to get the answer.\n",
        "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
        "\"\"\"\n",
        "base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
        "prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "agent = create_react_agent(\n",
        "    ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY, temperature=0),\n",
        "    tools,\n",
        "    prompt\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: Python_REPL\n",
            "Action Input: import sympy; x = sympy.Symbol('x'); eqn = sympy.Eq(3 * x**2 + 2*x -1, 0); result = sympy.solve([eqn], (x,)); print(result)\u001b[0m\u001b[36;1m\u001b[1;3m[(-1,), (1/3,)]\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "Final Answer: The roots of the quadratic function 3 * x**2 + 2*x -1 are -1 and 1/3.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1',\n",
              " 'output': 'The roots of the quadratic function 3 * x**2 + 2*x -1 are -1 and 1/3.'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.invoke({\"input\": \"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='The autoencoder has two parts:'),\n",
              " Document(page_content='Autoencoders are also very useful for other things, like understanding pictures or even helping'),\n",
              " Document(page_content='The autoencoder has two main parts: an encoder and a decoder.'),\n",
              " Document(page_content='So, the autoencoder machine is like a smart helper that can learn from its mistakes and find'),\n",
              " Document(page_content=\"An autoencoder is like a special machine that can do just that. It's like a puzzle game where you\"),\n",
              " Document(page_content='The autoencoder learns to do this by playing a game with itself. It starts with the big box of'),\n",
              " Document(page_content='autoencoder can be used to compress images of cats. When the images are reconstructed, they may not'),\n",
              " Document(page_content='So, the autoencoder uses the encoder to create a secret message from the broken pieces, and then'),\n",
              " Document(page_content='reconstructing the blocks. Eventually, it learns to compress the data into a smaller size while'),\n",
              " Document(page_content='1. The \"encoder\" part takes the big box of blocks (the input data) and squishes it down into a')]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retriever RAG Chatbot yapalım\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "retriever = search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
        "\n",
        "retriever.invoke(\"What is an Autoencoder?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Autoencoder, girdi verilerini daha küçük bir boyutta temsil etmeye çalışan bir yapay sinir ağı türüdür. Bu daha küçük temsil daha sonra orijinal girdi verilerini yeniden oluşturmak için kullanılır. Autoencoder'lar, örneğin veri sıkıştırma, özellik çıkarma ve gürültü giderme gibi çeşitli görevlerde kullanılır.\\n\\nTeşekkürler sorunuz için!\""
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"\"\"Use the following pieces of context with your own wisdom to answer the question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three paragraphs maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "You have to answer questions in Turkish, dont translate terms specific to English.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"Autoencoder nedir? \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
