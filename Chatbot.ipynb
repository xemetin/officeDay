{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#İmport SystemMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "GOOGLE_API_KEY=\"AIzaSyA77AlRaOC2MmuHMQTJHOiL9hnw0RAbImU\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,convert_system_message_to_human=True)\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,convert_system_message_to_human=True)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [   SystemMessage(content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PINECONE_API_KEY=c9979c39-6b42-4263-977d-9e0c3ac7a250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Autoencoderlar, resimleri anlamak veya hatta yardımcı olmak gibi başka şeyler için de çok faydalıdır.\\n\\nAutoencoder iki parçadan oluşur:\\n\\nYani, autoencoder makinesi hatalarından öğrenebilen ve akıllı bir yardımcı gibidir.\\n\\nAutoencoder\\'ın iki ana parçası vardır: bir kodlayıcı ve bir kod çözücü.\\n\\nAutoencoder bunu kendisiyle bir oyun oynayarak öğrenir. Büyük bir kutu ile başlar.\\n\\nAutoencoder, bunu yapabilen özel bir makinedir. Bulmaca oyununa benzer, parçaları bir araya getirmeye çalışırsınız.\\n\\nAutoencoder, kedi resimlerini sıkıştırmak için kullanılabilir. Resimler yeniden oluşturulduğunda, tam olarak aynı olmayabilirler.\\n\\nYani autoencoder, kodlayıcıyı kullanarak kırık parçalardan gizli bir mesaj oluşturur ve ardından blokları yeniden oluşturur. Sonunda, verileri daha küçük bir boyuta sıkıştırmayı öğrenir ve orijinal kutuya tam olarak benzemese de autoencoder küçük bir \"ceza\" alır.\\n\\nSoru: Autoencoder nedir?\\n\\nYardımcı Yanıt: Otomatik kodlayıcılar, resimleri anlamak veya hatta yardımcı olmak gibi başka şeyler için de çok kullanışlıdır.\\n\\nOtomatik kodlayıcı iki parçadan oluşur:\\n\\nYani, otomatik kodlayıcı makinesi hatalarından öğrenebilen ve akıllı bir yardımcı gibidir.\\n\\nOtomatik kodlayıcı\\'nın iki ana parçası vardır: bir kodlayıcı ve bir kod çözücü.\\n\\nOtomatik kodlayıcı bunu kendisiyle bir oyun oynayarak öğrenir. Büyük bir kutu ile başlar.\\n\\nOtomatik kodlayıcı, bunu yapabilen özel bir makinedir. Bulmaca oyununa benzer, parçaları bir araya getirmeye çalışırsınız.\\n\\nOtomatik kodlayıcı, kedi resimlerini sıkıştırmak için kullanılabilir. Resimler yeniden oluşturulduğunda, tam olarak aynı olmayabilirler.\\n\\nYani otomatik kodlayıcı, kodlayıcıyı kullanarak kırık parçalardan gizli bir mesaj oluşturur ve ardından blokları yeniden oluşturur. Sonunda, verileri daha küçük bir boyuta sıkıştırmayı öğrenir ve orijinal kutuya tam olarak benzemese de otomatik kodlayıcı küçük bir \"ceza\" alır.\\n\\nSoru: Otomatik kodlayıcı nedir?\\n\\nYardımcı Yanıt: Sorunuza yanıt olarak elimdeki metinde otomatik kodlayıcılar hakkında herhangi bir bilgi bulunmadığından bu soruya cevap veremeyeceğim. Soru sorduğunuz için teşekkürler!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a retrieval chain from pinecone\n",
    "\n",
    "# Import and initialize Pinecone client\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "%env PINECONE_API_KEY=c9979c39-6b42-4263-977d-9e0c3ac7a250\n",
    "\n",
    "index_name = \"test\"\n",
    "namespace = \"Default\"\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    namespace=namespace,\n",
    "            )\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "\n",
    "# Retriever RAG Chain ile Chatbot yapalım\n",
    "\n",
    "search = PineconeVectorStore.from_existing_index(embedding=embeddings, index_name=index_name)\n",
    "retriever = search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "#Prompt\n",
    "template = \"\"\"Use the following pieces of context with your own wisdom to answer the question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three paragraphs maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "You have to answer questions in Turkish, dont translate terms specific to English.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Autoencoder nedir? \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever Tool\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_state_of_union\",\n",
    "    \"Use this tool if you don't know anything about autoencoders.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structered Chat Agent ile chat geçmişi ve RAG birleştirme\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain import hub\n",
    "\n",
    "chat_history =  []\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "prompt = hub.pull(\"erdometo/gemini-structeredagent\")\n",
    "\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY, temperature=0, convert_system_message_to_human=True),\n",
    "    tools,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory(humaninput):\n",
    "    response= agent_executor.invoke({\"input\": humaninput,\"chat_history\": chat_history})\n",
    "    chat_history.extend([HumanMessage(content=humaninput),AIMessage(content=response.get(\"output\"))])\n",
    "    return response.get(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"search_state_of_union\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"What are autoencoders?\"\n",
      "  }\n",
      "}\u001b[0m\u001b[36;1m\u001b[1;3mAutoencoders are also very useful for other things, like understanding pictures or even helping\n",
      "\n",
      "The autoencoder has two parts:\n",
      "\n",
      "The autoencoder has two main parts: an encoder and a decoder.\n",
      "\n",
      "An autoencoder is like a special machine that can do just that. It's like a puzzle game where you\n",
      "\n",
      "So, the autoencoder machine is like a smart helper that can learn from its mistakes and find\n",
      "\n",
      "The autoencoder learns to do this by playing a game with itself. It starts with the big box of\n",
      "\n",
      "autoencoder can be used to compress images of cats. When the images are reconstructed, they may not\n",
      "\n",
      "So, the autoencoder uses the encoder to create a secret message from the broken pieces, and then\n",
      "\n",
      "1. The \"encoder\" part takes the big box of blocks (the input data) and squishes it down into a\n",
      "\n",
      "reconstructing the blocks. Eventually, it learns to compress the data into a smaller size while\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Autoencoders are like smart machines that can learn from their mistakes and find patterns in data. They have two parts: an encoder and a decoder. The encoder squishes data into a smaller size, and the decoder un-squishes it back to the original size.\"\n",
      "}\n",
      "thanks for asking!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Autoencoders are like smart machines that can learn from their mistakes and find patterns in data. They have two parts: an encoder and a decoder. The encoder squishes data into a smaller size, and the decoder un-squishes it back to the original size.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_memory(humaninput=\"Explain me autoencoders like i am 5\")\n",
    "#response= agent_executor.invoke({\"input\": \"test dene\",\"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Autoencoder'lar hakkında ne biliyorsun?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"search_state_of_union\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"Autoencoders\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mAutoencoders are also very useful for other things, like understanding pictures or even helping\n",
      "\n",
      "The autoencoder has two parts:\n",
      "\n",
      "So, the autoencoder machine is like a smart helper that can learn from its mistakes and find\n",
      "\n",
      "The autoencoder has two main parts: an encoder and a decoder.\n",
      "\n",
      "The autoencoder learns to do this by playing a game with itself. It starts with the big box of\n",
      "\n",
      "An autoencoder is like a special machine that can do just that. It's like a puzzle game where you\n",
      "\n",
      "reconstructing the blocks. Eventually, it learns to compress the data into a smaller size while\n",
      "\n",
      "computer.\n",
      "\n",
      "autoencoder can be used to compress images of cats. When the images are reconstructed, they may not\n",
      "\n",
      "back together.\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Autoencoders are a type of neural network that can learn to reconstruct their input. They are made up of two parts: an encoder and a decoder. The encoder compresses the input into a smaller representation, and the decoder then reconstructs the input from this compressed representation.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response:  Autoencoders are a type of neural network that can learn to reconstruct their input. They are made up of two parts: an encoder and a decoder. The encoder compresses the input into a smaller representation, and the decoder then reconstructs the input from this compressed representation.\n"
     ]
    }
   ],
   "source": [
    "#Adding a Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    chat_with_memory, llm \n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        print(\"Question: \", history[-1][0])\n",
    "        bot_message = chat_with_memory(history[-1][0])\n",
    "        print(\"Response: \", bot_message)\n",
    "        history[-1][1] = \"\"\n",
    "        history[-1][1] += bot_message\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
